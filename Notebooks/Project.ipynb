#importing necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#REading training data
df = pd.read_csv("Train_Data.csv") 

df.shape

df.head()

df.isnull().sum() #checking for existence and frequency of null values

OBSERVATION : The columns m0,m1 and m2 have no null values, so there will be no need of imputation for these columns

Skew() tells us how normally distributed a feature is . If skew value ranges from -0.5 to 0.5 , then it has a low skew in which case we can replace the null values by median , otherwise for data with high skew values , median is preferred for imputation


df.skew(axis = 0) 

The "ld" column has high skew value , so we will use median but let's double check that using box plot

fig,ax = plt.subplots(figsize=(10,6))
sns.boxplot(df.ld)

Clearly , we can not use mean imputation as there are large number of outliers

#ld column is heavily skewed as 0.95 > 0.5 so median imputation is preferred

med = df.ld.median()
df.ld.fillna(med,inplace = True)
df.ld.isnull().sum()

The rest of the columns are not heavily skewed , so mean imputation works for all of them

#Visualising that these columns are normally distributed

# Select the columns from 'm3' to 'm14'
columns = ['m3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']

# Iterate over each column and plot individual histograms
for column in columns:

    sns.histplot(data=df[column], bins='auto', kde=True)
    plt.xlabel('Values')
    plt.ylabel('Frequency')
    plt.title(f'Histogram of {column}')
    plt.show()

columns_to_fill = ['m3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']

# Fill NaN values with column means
df[columns_to_fill] = df[columns_to_fill].fillna(df[columns_to_fill].mean())
df.isnull().sum()

# All null values successfully removed

Now , the two categorical columns 'pc' and 'ma' need to be encoded here as only numerical data can be modelled in this case where I am using XGBoost model

df = pd.get_dummies(df,columns = ['pc','ma'],prefix = ['pc','ma'])
df.head()

# Feature Scaling can significantly increase accuracy of model but since it is XGBoost which is almost insesitive to feature scaling , we are scaling only the "ld" column , not all of them

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

ld_column = df['ld'].values.reshape(-1, 1)

# Apply min-max scaling
scaled_ld = scaler.fit_transform(ld_column)

# Update the 'ld' column in the DataFrame with the scaled values
df['ld'] = scaled_ld

After building the model , feature_importance suggests that these 4 are the least relevant columns , thus dropping them here before training itself 

df = df.drop(['m0','m1','m2','m12'],axis = 1)

Defining the data(X) and target(Y) columns 

X = df.drop('pred',axis = 1)
y = df.pred

Splitting the data into training and testing set to build the model 

from sklearn.model_selection import train_test_split

X_train , X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 10)

# Now importing test data


new_data = pd.read_csv("Test_Data.csv")

new_data.head()

# Cleaning the test data exactly the same way as training data

new_data.skew(axis = 0)

# Assuming your DataFrame is called 'df4'
columns_to_fill = ['m3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']

# Fill NaN values with column means
new_data[columns_to_fill] = new_data[columns_to_fill].fillna(new_data[columns_to_fill].mean())
new_data.isnull().sum()

new_data = pd.get_dummies(new_data,columns = ['pc','ma'],prefix = ['pc','ma'])
new_data.head()

med_n = new_data.ld.median()
new_data.ld.fillna(med,inplace = True)
new_data.ld.isnull().sum()

new_data.columns

# TEST DATA CLEANING OVER 

Now , we build our model . Since it is a binary classification problem , we use a classifier - XGBClassifier

from xgboost import XGBClassifier

# Create an XGBoost classifier with L1 regularization
xgb_clf = XGBClassifier(reg_alpha=2.0, objective='binary:logistic',n_estimators = 300)

# Train the classifier
xgb_clf.fit(X_train, y_train)

# Make predictions
pred = xgb_clf.predict(X_test)

The next two cells were operated before dropping the columns m0,m1,m2 and m12 , so the results below are different .For better understanding , run these two cells before dropping the columns

importance = xgb_clf.feature_importances_
feature_names = X_train.columns

# Create a DataFrame to store feature importance
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.show()

feature_importance_df

Clear enough , m0,m1, m2 and m12 have very less importance so they are dropped from both the training and testing set

new_data = new_data.drop(['m0','m1','m2','m12'],axis = 1)

Now , predicting the unknown test data set using our classifier

pred = xgb_clf.predict(new_data.drop('index',axis = 1))

#checking unique values of np.unique
np.unique(pred)

#checking the frequency of 1
np.count_nonzero(pred == 1)

#Save predictions as a data frame
pred_df = pd.DataFrame({'pred': pred})

# Save predictions to a CSV file
pred_df.to_csv('predictions_svc_exp_minmax_dropped_1.csv', index=False)
